<!doctype html>
<html lang="en">
    <head>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="icon" href="img/log.png" type="image/png">
        <title>Curate</title>
        <!-- Bootstrap CSS -->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="vendors/linericon/style.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">
        <link rel="stylesheet" href="vendors/owl-carousel/owl.carousel.min.css">
        <link rel="stylesheet" href="vendors/lightbox/simpleLightbox.css">
        <link rel="stylesheet" href="vendors/nice-select/css/nice-select.css">
        <link rel="stylesheet" href="vendors/animate-css/animate.css">
        <link rel="stylesheet" href="vendors/flaticon/flaticon.css">
        <!-- main css -->
        <link rel="stylesheet" href="css/style.css">
        <link rel="stylesheet" href="css/responsive.css">
    </head>
    <body>
        
        <!--================Header Menu Area =================-->
        <!--================Header Menu Area =================-->
        
        <!--================Home Banner Area =================-->

        <!--================End Home Banner Area =================-->
        
        <!--================Portfolio Details Area =================-->
        <section class="portfolio_details_area p_120"  style="background: #f9f9ff; padding-bottom: 50px;">
            <div class="container">
                <div class="portfolio_details_inner">
                    <div class="row">
                        <div class="left_img"">
                            <img class="img-fluid" src="img/curate/curate.png" alt="CURATE" style="height: 350px; padding-right: 30px;">
                        </div>
                        <div class="col-md-6">
                            <div style="padding-left: 0px;" class="portfolio_right_text">
                                <h4>Curate Solutions</h4>
                                <p><a href="https://www.curatesolutions.com/">Curate Solutions </a>scrapes through minutes and agenda documents from local meetings of hundreds of counties across the country each week. The Madison based startup scans through the collected data using a mixture of machine learning/language processing models and a human data analysis team to retrieve information valuable to customers. Customers may include construction companies looking for new projects to bid on etc. During my time at the company, I worked on building, training and deploying machine learning models on large scale textual datasets to extract data valuable to customers.</p>
                                <ul class="list">
                                    <li><span>Time period</span>:  May 2018 - May 2019</li>
                                    <li><span>Location</span>:  Madison, WI</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="portfolio_details_area p_120" style="padding-top: 50px;">
            <div class="container">
                <h4 style="color: #222222; margin-bottom:40px; font-size: 30px; text-align: center;">Software Engineer</h4>
                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Built a text vectorization framework using the word2vec distributed representation algorithm</h3>
                        <ul style="list-style: none; float: right; padding-left: 20px">
                            <li><img src="img/curate/word2vec.png" style="height: 220px;" alt="W2V"></li>
                            <li><img src="img/curate/skipGram.png" style="height: 400px; padding-top: 50px; padding-left: 55px; " alt="SKIPgRAM"></li>
                        </ul>
                        <p>One of the fundamental challenges in using machine learning on textual data was contextually representing this data in a machine-readable numeric format. To do this each word was converted to a 300-dimensional vector using the pre trained <a href="https://nlp.stanford.edu/projects/glove/">GloVe vectors</a>. GloVe vectors are created using the unsupervised <a href="https://arxiv.org/pdf/1310.4546.pdf">word2vec</a> learning algorithm. This algorithm uses the continuous Skip-Gram architecture to predict the window of contextually suitable words around a specific pivot word. This is done by taking a corpus of textual data and assigning each word a random 300-dimensional vector. Then a window size is chosen and every word in the window is used to predict the neighboring words. Every word is assigned a random 300-dimensional vector prior to training. A <a href="https://arxiv.org/pdf/1411.2738.pdf">neural network</a> is used for the prediction task in which backpropagation is used for training. By doing this, words that occur in similar contexts or close to each other end up getting placed close to one another in vector space.</p>

                        <p style="padding-top: 10px;"><a href="https://spacy.io/">spaCy</a> provides a way to <a href="https://spacy.io/usage/vectors-similarity">efficiently implement word2vec</a> along with many more language processing functions such as tokenization, lemmatization, part of speech tagging etc. and was thus used to convert text data into 300-dimensional word vectors in production. The pre trained 300-dimensional spaCy GloVe vectors were improved on by conducting further training using a custom corpus of the company’s textual data. This was done with the help of <a href="https://radimrehurek.com/gensim/models/word2vec.html">Gensim</a> which uses the word2vec Skip-Gram technique as discussed above to train word vectors for a corpus of data. But instead of using random vectors to initialize training as discussed above, the pre trained 300-dimensional spaCy GloVe vectors were used to initialize each word vector in the corpus. The corpus was then trained using the word2vec Skip-Gram technique which resulted in word vectors that better represented the contexts of the corpus it was trained on.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Built a distributed Spark-Kubernetes framework to run machine learning algorithms on large datasets</h3>
                        <img src="img/curate/kube.png" style="float: right; height: 320px; padding-left: 10px; padding-top: 15px;" alt="KUBE">
                        <p style="padding-top: 10px; max-width: 640px;">Once the input vector creation method was ready, the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit-learn classifier</a> with the right hyperparameters had to be chosen. This was done by running a grid search across multiple models with varying hyperparameters. Training and testing the list of viable models was done using <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">Kubernetes running Apache Spark.</a> The models with their corresponding hyperparameters were split up by the Spark driver pod into a list of jobs distributed among available executor pods. Each executor pod retrieved labeled data stored in <a href="https://www.mongodb.com/">mongoDB</a>, vectorized the data using customized spaCy 300-dimension vectors, used a training set of data to train a single model and used a test set of the data to generate precision recall scores. The hyperparameters of models with the top 10 precision recall scores were saved. Once the model with best precision recall scores were identified, the model was trained on a larger dataset using the Spark-Kubernetes framework. The Spark driver pod split up the training data into jobs and each executor pod partially trained the classifier model on a single portion of data. Once the models were fully trained, they were stored in <a href="https://aws.amazon.com/s3/">AWS S3 buckets</a> for use in production along with metadata related to the model.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Trained and deployed a URL classifier to find valuable minutes and agenda webpage</h3>
                        <img src="img/curate/url.PNG" style="float: right; height: 400px; padding-left: 20px" alt="URL">
                        <p>The first step of the entire data gathering system involved finding the right webpages (URLs) to scrape. A dataset of URLs containing minutes and agenda data vs ones that do not (good vs bad URLs) was used to build a binary classifier. To identify the parts of the URL that could be used as features, the good and bad URLs were analyzed using basic statistical analysis techniques. The resulting analysis showed that the protocol and domain name sections of the URL did not help in differentiating good vs bad URLs. These sections of the URLs were removed in preprocessing and only the path section of the URL was used as an input variable for the binary classifier. As an additional data scrubbing step, the path portion of the URL was further preprocessed using spaCy’ s tokenization, lemmatization and various <a href="https://docs.python.org/3/library/re.html">regular expressions</a>. The data was then vectorized using pre trained spacy 300-dimensional GloVe vectors which were customized by further training the entire URL corpus using Gensim as discussed above. A grid search was conducted using the above-mentioned distributed Spark-Kubernetes framework to find the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit-learn classifier</a>. Once the best classifier was found, it was trained and deployed in production using the same distributed framework as mentioned above.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Trained and deployed a classifier to eliminate syntactic junk data</h3>
                        <p> The next task was to eliminate data that was corrupted by the OCR (optical character recognition) extractor. OCR junk consisted of data points (chunks of textual data) that contained junk characters and thus did not make grammatic sense. The first step was to vectorize textual data point. The previously discussed 300 dimensional spaCy vectors could not be used as these vectors represented words in context and could not interpret junk text logically. To numerically represent junk vs not junk characters, a custom 10-dimensional vector was constructed. Each vector dimensions consisted of character specific attributes of the data point such as number of alpha to alpha transitions, alpha to non-alpha transitions, alpha to numeric transitions, pos tag count etc. Once the data was vectorized, a simple binary classifier approach could not be used as there was no labeled OCR junk data to train on. To generate a large dataset of junk data, the <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means unsupervised learning algorithm</a> was first used to separate junk and not junk data into large homogenous cluster. The clusters were analyzed using TensorFlow's visualization tool <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard">TensorBoard</a>. Each cluster of data was then hand labelled as containing junk or not junk using a custom in-house <a href="https://angular.io/">Angular</a> data labelling web application. The web application was created in such a way that labelling a cluster of data points as junk or not junk resulted in a flag being set in a MongoDB collection for all the data points in the cluster. This way a large amount of labelled data was created in a short period of time. The newly created labelled data was vectorized using the 10-dimensional custom vectors and grid search was conducted using the distributed spark framework to find the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit-learn classifier</a> with optimal hyperparameters. The best model was trained and deployed in production using the above-mentioned distributed framework.</p>
                        <img src="img/curate/tensorboard.jpg" style="margin-left: auto; margin-right: auto; display: block; height: 350px; padding-top: 20px;" alt="TENSOR-BOARD">
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Trained and deployed a classifier to identify datapoints valuable to customers</h3>
                        <!-- <img src="" style="float: right; height: 200px;" alt="COMING SOON"> -->
                        <p>This step involved building a classifier to rate data points on how valuable they would be to clients. Once the junk data was removed to prevent bad samples in the data set, training this classifier was relatively straightforward as there were labels of data points previously sent to customers. The data points previously sent to customers made up the positive samples while data points not sent made up negative samples once the OCR junk was removed. The <a href="https://spacy.io/usage/vectors-similarity">300-dimension spaCy vectors</a> customized using <a href="https://radimrehurek.com/gensim/models/word2vec.html">gensim word2vec</a> was used to vectorize data points. Customizing pretrained spaCy 300-dimensional GloVe vectors for the data point corpus using genism greatly improved the contextual representation of each word vector for the data point corpus. This helped train classification models with superior precision recall scores. Once the custom vectorization technique was created, grid search was conducted using the distributed spark framework to find the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit-learn classifier</a> with optimal hyperparameters. The best model was trained and deployed in production using the above-mentioned distributed framework.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <!-- <h3 style="" class="mb-30 title_color">Most of the backend code for model training and deployment was written in python. Apache Spark and kubernetes were used to train models on large datasets. MongoDB and AWS S3 were used to store data and machine learning models respectively. Scikit-learn, SpaCy and Gensim were used in building the machine learning framework. Angular was used for the front-end website components when designing data labelling tools.</h3> -->
                        <ul style="list-style: none;">
                            <li>
                                <h3 style="" class="mb-30 title_color">The back end code for model training and deployment was written in Python 3.</h3>
                                <img src="img/curate/python.png" style="height: 130px;" alt="">
                            </li>
                            <li style="padding-top: 30px;">
                                <h3 style="" class="mb-30 title_color">Apache Spark and Kubernetes were used to train models on large datasets.</h3>
                                <img src="img/curate/spark.png" style="height: 120px;" alt="">
                            </li>
                            <li style="padding-top: 30px;">
                                <h3 style="" class="mb-30 title_color">MongoDB and AWS S3 were used to store data and machine learning models respectively.</h3>
                                <ul style="list-style: none; padding-left: 0px">
                                    <li><img src="img/curate/aws.png" style="height: 100px; float: left;" alt=""></li>
                                    <li><img src="img/curate/mongo.png" style="height: 100px; padding-left: 30px;" alt=""></li>
                                </ul>
                            </li>
                            <li style="padding-top: 30px;">
                                <h3 style="" class="mb-30 title_color">Scikit-learn, SpaCy and Gensim were used in building the machine learning framework.</h3>
                                <ul style="list-style: none;">
                                    <li><img src="img/curate/scikit.png" style="height: 80px; float: left;" alt=""></li>
                                    <li><img src="img/curate/spacy.png" style="height: 80px; float: left; padding-left: 30px;" alt=""></li>
                                    <li><img src="img/curate/gensim.png" style="height: 80px; padding-left: 30px;" alt=""></li>
                                </ul>
                            </li>
                            <li style="padding-top: 30px;">
                                <h3 style="" class="mb-30 title_color">Angular 5 was used for the front end website components when designing data labelling tools.</h3>
                                <img src="img/curate/angular.png" style="height: 150px;" alt="">
                            </li>
                        </ul>
                    </div>
                </div>

            </div>
        </section>
        <!--================End Portfolio Details Area =================-->
        
        <!--================Footer Area =================-->
        
        <!--================End Footer Area =================-->
        
        
        
        
        <!-- Optional JavaScript -->
        <!-- jQuery first, then Popper.js, then Bootstrap JS -->
        <script src="js/jquery-3.2.1.min.js"></script>
        <script src="js/popper.js"></script>
        <script src="js/bootstrap.min.js"></script>
        <script src="js/stellar.js"></script>
        <script src="vendors/lightbox/simpleLightbox.min.js"></script>
        <script src="vendors/nice-select/js/jquery.nice-select.min.js"></script>
        <script src="vendors/isotope/imagesloaded.pkgd.min.js"></script>
        <script src="vendors/isotope/isotope-min.js"></script>
        <script src="vendors/owl-carousel/owl.carousel.min.js"></script>
        <script src="js/jquery.ajaxchimp.min.js"></script>
        <script src="js/mail-script.js"></script>
        <script src="vendors/counter-up/jquery.waypoints.min.js"></script>
        <script src="vendors/counter-up/jquery.counterup.min.js"></script>
        <script src="js/theme.js"></script>
    </body>
</html>