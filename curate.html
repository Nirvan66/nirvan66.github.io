<!doctype html>
<html lang="en">
    <head>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="icon" href="img/log.png" type="image/png">
        <title>Curate</title>
        <!-- Bootstrap CSS -->
        <link rel="stylesheet" href="css/bootstrap.css">
        <link rel="stylesheet" href="vendors/linericon/style.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">
        <link rel="stylesheet" href="vendors/owl-carousel/owl.carousel.min.css">
        <link rel="stylesheet" href="vendors/lightbox/simpleLightbox.css">
        <link rel="stylesheet" href="vendors/nice-select/css/nice-select.css">
        <link rel="stylesheet" href="vendors/animate-css/animate.css">
        <link rel="stylesheet" href="vendors/flaticon/flaticon.css">
        <!-- main css -->
        <link rel="stylesheet" href="css/style.css">
        <link rel="stylesheet" href="css/responsive.css">
    </head>
    <body>
        
        <!--================Header Menu Area =================-->
        <!--================Header Menu Area =================-->
        
        <!--================Home Banner Area =================-->

        <!--================End Home Banner Area =================-->
        
        <!--================Portfolio Details Area =================-->
        <section class="portfolio_details_area p_120"  style="background: #f9f9ff; padding-bottom: 50px;">
            <div class="container">
                <div class="portfolio_details_inner">
                    <div class="row">
                        <div class="left_img"">
                            <img class="img-fluid" src="img/curate/curate.png" alt="CURATE" style="height: 350px; padding-right: 30px;">
                        </div>
                        <div class="col-md-6">
                            <div style="padding-left: 0px;" class="portfolio_right_text">
                                <h4>Curate Solutions</h4>
                                <p><a href="https://www.curatesolutions.com/">Curate Solutions </a>scrapes through minutes and agenda documents from local meetings of hundereds of counties across the country each week. The Madison based startup scans through the collected data using a mixture of machine learning/language processing models and a human data analysis team to retrive information valuable to customers. Customers may include construction companies looking for new projects to bid on etc. During my time at the company, I worked on building, training and deploying machine learning models on large scale textual datasets to extract data valuable to customers.</p>
                                <ul class="list">
                                    <li><span>Time period</span>:  May 2018 - May 2019</li>
                                    <li><span>Location</span>:  Madison, WI</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="portfolio_details_area p_120" style="padding-top: 50px;">
            <div class="container">
                <h4 style="color: #222222; margin-bottom:40px; font-size: 30px; text-align: center;">Software Engineer</h4>
                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Used spaCy for per-processing and word vectorization of textual data.</h3>
                        <ul style="list-style: none; float: right;">
                            <li><img src="img/curate/word2vec.png" style="height: 220px;" alt="W2V"></li>
                            <li><img src="img/curate/skipGram.png" style="height: 400px; padding-top: 50px; padding-left: 55px; " alt="SKIPgRAM"></li>
                        </ul>
                        <p>One of the fundamental challenges in using machine learning on textual data was converting this data into a machine readable numeric format. To do this each word was converted to a 300 dimensinal vector using the pre trained <a href="https://nlp.stanford.edu/projects/glove/">GloVe vectors</a>. GloVe vectors are created using the unsupervise <a href="https://arxiv.org/pdf/1310.4546.pdf">word2vec</a> learning algorithm. This algorithm uses the continuous skipGram architecture to predict the window of context words around a specific pivot word. This is done by taking a corpus of textual data and assigning each word a random 300 dimensional vector. Then a window size is chosen and every word in the window is used to predict the neighbouring words. A <a href="https://arxiv.org/pdf/1411.2738.pdf">neural network</a> is used for the prediction task in which backpropagation is used for training. By doing this, words that occur in similar contexts or close to each other end up getting placed close to one another in vector space.</p>

                        <p style="padding-top: 10px;"><a href="https://spacy.io/">spaCy</a> provides a way to <a href="https://spacy.io/usage/vectors-similarity">efficently impliment word2vec</a> along with many more language processing functions such as tokenization, lemmatization, part of speech tagging etc. and was thus used to convert text data into 300 dimensional word vectors in production. The pre trained 300 dimensional spaCy GloVe vectors were improved on by conducting furthur training using a custom corpus of textual data. This was done with the help of <a href="https://radimrehurek.com/gensim/models/word2vec.html">Gensim</a> which uses the word2vec skipGram technique as discussed above to train word vectors for a corpus of data. But instead of using random vectors to initialize training as discussed above, the pre trained 300 dimensional spaCy GloVe vectors were used to initialize each word vector in the corpus. The corpus was then trained using the word2vec skipGram technique which resulted in word vectors that better represented the contexts of the corpus it was trained on.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Built a distributed spark kubernetes framework to run machine learning algorithms on large datasets.</h3>
                        <img src="" style="float: right; height: 200px;" alt="COMING SOON">
                        <p style="padding-top: 10px;">Once input vectors creation method was ready, the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit classifier</a> with the right hyperparameters had to be chosen. This was done by running a grid search across multiple models with varying hyperparameters. Training and testing the list of viable models was done using <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">kubernetes running apache spark.</a> The models with their corresponding hyperparameters were split up by the kubernetes driver pod into a list of jobs distrubuted among available excecutor pods. Each executor pod retrived labeled url data stored in <a href="https://www.mongodb.com/">mongoDB</a>, vectorized the url data using customized spaCy 300 dimension vectors, used a traning set of data to train a single model and used a test set of the data to generate precision recall scores. The hyperparameters of models with the top 10 percision recall scores were saved. Once the model with best precision recall scores were identified, the model was trained on a larger dataset using a spark-kubernetes framework. The kubernetes driver pod split up the training data into jobs and each executor pod partialy trained the classifier model on a single portion of data. Once the models were fully trained, they were stored in <a href="https://aws.amazon.com/s3/">AWS S3 buckets</a> for use in production along with metadata related to the model.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Trained and deployed a URL classifier to find valuable minutes and agenda webpage.</h3>
                        <img src="" style="float: right; height: 200px;" alt="COMING SOON">
                        <p>The first step of the entire data gathering system involved finding the right webpages (urls) to scrape. A dataset of urls containing minutes and agenda data vs ones that do not (good vs bad urls) was used used to build a binary classifier. To identify the parts of the url that could be used as input variables, the good and bad urls were analysed using basic data science related statistical analysis techniques. The resulting analysis showed that the protocol and domain name sections of the url did not help in differentiating good vs bad urls. These sections of the urls were removed in pre processing and only the path section of the url was used as an input variable for the binary classifier. As an additional data scrubbing step, the path portion of the url furthur pre processed usign certain spaCy tools and various <a href="https://docs.python.org/3/library/re.html">regular expressions</a>. The data was then vectorised using pre trained 300 dimensional spacy word GloVe vectors which were customized by further training of the entire url corpus using gensim as discussed above. A grid search was conducted usign the above mentioned distributed spark kubernetes framework to find the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit classifier</a>. Once the best classifier was found, it was trained and deployed in production using the same distributed framework as mentioned above.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Trained and deployed classifier to eliminate syntactic junk data.</h3>
                        <img src="" style="float: right; height: 200px;" alt="COMING SOON">
                        <p> The next task was to eliminate data that was corrupted by the OCR(optical charachter recognition) data extractor. OCR junk consisted of data points (chunks of textual data) that containted junk charachters and thus did not make grammatic sense. The first step was to vectorize textual data point. The previously discussed 300 dimensional spaCy vectors could not be used as these vectors represented words in context and could not interpret junk text logically. To numerically represent junk vs not junk charachters, a custom 10 dimensional vector was constructed. Each vector dimensions consisted of charachter specific attributes of the data point such as number of alpha to alpha transitions, alpha to non-alpha transitions, alph to numeric transitions, pos tag count etc. Once data was vectorized, a simple binary classifier approach could not be used as there was no labeled OCR junk data to go off of. To generate a large dataset of junk data, the <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means unsupervised learning algorithm</a> was first used to separate junk and not junk data into large homogenous cluster. The clusters were analysed using TensorFlow's visualization tool <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard">TensorBoard</a>. Each cluster of data was then hand labelled as containing junk or not junk using a custom in-house <a href="https://angular.io/">Angular</a> data labelling web application. The web application was created in such a way that labelling a cluster of data points as junk or not junk resulted in a flag being set in a mongoDB collection for all the data points in the cluster. This way a large amount of labelled data was created in a short period of time. The newly created labelled data was vectorized using the 10 dimensional custom vectors and grid search was conducted using the distributed spark framework to find the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit classifier</a> with optimal hyperparameters. The model best model was trained and deployed in production using the above mentioned distributed framework.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Trained and deployed classifer to identify datapoints valuable to customers.</h3>
                        <img src="" style="float: right; height: 200px;" alt="COMING SOON">
                        <p>This involved building a classifier to rate data points on how valuable they would be to clients. Once the junk data was removed to prevent bad samples in the data set, training this classifier was relatively straightforward as there were lables of data points previously sent to customers. The data points previously sent to customeres made up the positive samples while data points not sent made up negative samples once the OCR junk was removed. The <a href="https://spacy.io/usage/vectors-similarity">300 dimension spaCy vectors</a> customised using <a href="https://radimrehurek.com/gensim/models/word2vec.html">gensim word2vec</a> was used to vectorize data points. Customizing pretrained spaCy 300 dimensional GloVe vectors for the data point corpus using gensim greatly improved the contextual representaion of each word vector for the data point corpus. This helped train classification models with superior precision recal scores. Once the custom vectorization technique was created, grid search was conducted using the distributed spark framework to find the best <a href="https://scikit-learn.org/stable/supervised_learning.html">scikit classifier</a> with optimal hyperparameters. The model best model was trained and deployed in production using the above mentioned distributed framework.</p>
                    </div>
                </div>

                <div class="col-md-12" style="padding-right: 0px; padding-left: 0px;">
                    <div class="feature_item" style = "background: #eee; border-left: 10px solid #8490ff; overflow: auto;">
                        <h3 style="" class="mb-30 title_color">Development was done in a .NET framework on Microsoft Visual Studio. While the backbone of the heatmap layer was written in C#, XAML was used to render the visual component.</h3>
                        <ul style="list-style: none;">
                            <li><img src="img/epg/visual_studio.jpg" style="height: 150px; float: left;" alt=""></li>
                            <li><img src="img/epg/csh.png" style="height: 150px; float: left; padding-left: 10px;" alt=""></li>
                            <li><img src="img/epg/xaml.jpg" style="height: 150px; float: left; padding-left: 10px;" alt=""></li>
                        </ul>
                    </div>
                </div>

            </div>
        </section>
        <!--================End Portfolio Details Area =================-->
        
        <!--================Footer Area =================-->
        
        <!--================End Footer Area =================-->
        
        
        
        
        <!-- Optional JavaScript -->
        <!-- jQuery first, then Popper.js, then Bootstrap JS -->
        <script src="js/jquery-3.2.1.min.js"></script>
        <script src="js/popper.js"></script>
        <script src="js/bootstrap.min.js"></script>
        <script src="js/stellar.js"></script>
        <script src="vendors/lightbox/simpleLightbox.min.js"></script>
        <script src="vendors/nice-select/js/jquery.nice-select.min.js"></script>
        <script src="vendors/isotope/imagesloaded.pkgd.min.js"></script>
        <script src="vendors/isotope/isotope-min.js"></script>
        <script src="vendors/owl-carousel/owl.carousel.min.js"></script>
        <script src="js/jquery.ajaxchimp.min.js"></script>
        <script src="js/mail-script.js"></script>
        <script src="vendors/counter-up/jquery.waypoints.min.js"></script>
        <script src="vendors/counter-up/jquery.counterup.min.js"></script>
        <script src="js/theme.js"></script>
    </body>
</html>